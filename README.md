# llm-cache

This repo contains a set of demo samples to implement caching techniques with LangChain and OpenAI models, including the analysis of its performance. The samples are contained in the ll_cache_demo.ipynb Jupyter Notebook file.

## Setup

pip install -r requirements.txt